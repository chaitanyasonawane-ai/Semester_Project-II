{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "hf_cGmzeQvNO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0tkSh-QuW3s",
        "outputId": "9b0696ac-2747-454b-9763-7a2c46640a22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/YoutubeCommentsDataSet.csv')\n",
        "print(\"Columns in dataset:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "VvNWoiuDRjlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac97fed-a22b-41b4-b29f-44f63575bca1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: ['Comment', 'Sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in 'Comment' or 'Sentiment'\n",
        "df.dropna(subset=['Comment', 'Sentiment'], inplace=True)"
      ],
      "metadata": {
        "id": "K-7DotCZ0J5k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = str(text)  # Ensure text is string\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    return text.strip().lower()  # Convert to lowercase\n",
        "\n",
        "# Apply the clean_text function to create the 'clean_text' column\n",
        "df['clean_text'] = df['Comment'].apply(clean_text)"
      ],
      "metadata": {
        "id": "om2odOjGRnzo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization (Ensure this runs after 'clean_text' is created)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized_tokens'] = df['clean_text'].apply(\n",
        "    lambda x: [lemmatizer.lemmatize(word) for word in word_tokenize(x)]\n",
        ")"
      ],
      "metadata": {
        "id": "9vF5JJWmRuGP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Just Example\n",
        "sample_comment = \"Check out this video: https://youtu.be/FLZvOKSCkxY?si=-fVMETgk-_q3X2Wi  and subscribe!\"\n",
        "cleaned_sample = clean_text(sample_comment)\n",
        "print(\"Cleaned Sample:\", cleaned_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvHKC1vMRy6r",
        "outputId": "aa80eba2-55b7-4424-ccb4-a775b311d457"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Sample: check out this video   and subscribe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean overall Dataset\n",
        "df['clean_text'] = df['Comment'].apply(clean_text)"
      ],
      "metadata": {
        "id": "EEcPO68sSLlc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "source": [
        "# Process the data in datsets\n",
        "print(\"\\nProcessed DataFrame:\")\n",
        "print(df[['Comment', 'clean_text', 'lemmatized_tokens']].head()) # Changed 'stemmed_tokens' to 'lemmatized_tokens'"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqVL_KQ9v5cs",
        "outputId": "45cfd25c-29ee-4184-b522-fd2dc8b142bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed DataFrame:\n",
            "                                             Comment  \\\n",
            "0  lets not forget that apple pay in 2014 require...   \n",
            "1  here in nz 50 of retailers don’t even have con...   \n",
            "2  i will forever acknowledge this channel with t...   \n",
            "3  whenever i go to a place that doesn’t take app...   \n",
            "4  apple pay is so convenient secure and easy to ...   \n",
            "\n",
            "                                          clean_text  \\\n",
            "0  lets not forget that apple pay in  required a ...   \n",
            "1  here in nz  of retailers dont even have contac...   \n",
            "2  i will forever acknowledge this channel with t...   \n",
            "3  whenever i go to a place that doesnt take appl...   \n",
            "4  apple pay is so convenient secure and easy to ...   \n",
            "\n",
            "                                   lemmatized_tokens  \n",
            "0  [let, not, forget, that, apple, pay, in, requi...  \n",
            "1  [here, in, nz, of, retailer, dont, even, have,...  \n",
            "2  [i, will, forever, acknowledge, this, channel,...  \n",
            "3  [whenever, i, go, to, a, place, that, doesnt, ...  \n",
            "4  [apple, pay, is, so, convenient, secure, and, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization with stop words removal\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['clean_text'])  # Convert text to numerical features\n",
        "y = df['Sentiment']  # Target variable"
      ],
      "metadata": {
        "id": "ZsXiky6zQcw1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PkcARwhewY9-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Naive Bayes\n",
        "model = MultinomialNB()  # Create a MultinomialNB object\n",
        "model.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "y_pred = model.predict(X_test)  # Make predictions"
      ],
      "metadata": {
        "id": "3iFB8HoAwe0U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS3TXZ_Qwn2C",
        "outputId": "0004feb1-e3bf-4e46-8af6-c3e38aa024aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6492146596858639\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.01      0.01       275\n",
            "     neutral       0.80      0.11      0.20       582\n",
            "    positive       0.64      0.99      0.78      1435\n",
            "\n",
            "    accuracy                           0.65      2292\n",
            "   macro avg       0.82      0.37      0.33      2292\n",
            "weighted avg       0.73      0.65      0.54      2292\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and vectorizer using pickle\n",
        "with open(\"sentiment_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as vectorizer_file:\n",
        "    pickle.dump(vectorizer, vectorizer_file)"
      ],
      "metadata": {
        "id": "PMP8_8jv1pJ0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load model and vectorizer for prediction\n",
        "def load_model():\n",
        "    with open(\"sentiment_model.pkl\", \"rb\") as model_file:\n",
        "        loaded_model = pickle.load(model_file)\n",
        "    with open(\"tfidf_vectorizer.pkl\", \"rb\") as vectorizer_file:\n",
        "        loaded_vectorizer = pickle.load(vectorizer_file)\n",
        "    return loaded_model, loaded_vectorizer"
      ],
      "metadata": {
        "id": "07RVAjXA1x70"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict sentiment for the entire dataset (New Addition)\n",
        "df['predicted_sentiment'] = model.predict(X)  # Apply model to all comments"
      ],
      "metadata": {
        "id": "_zZDlYcF2cmZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the first few rows with predictions (New Addition)\n",
        "print(df[['Comment', 'Sentiment', 'predicted_sentiment']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GP1Ti52fhM",
        "outputId": "99aa98e4-5df2-4bf3-8806-d7715548dfaa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Comment Sentiment  \\\n",
            "0  lets not forget that apple pay in 2014 require...   neutral   \n",
            "1  here in nz 50 of retailers don’t even have con...  negative   \n",
            "2  i will forever acknowledge this channel with t...  positive   \n",
            "3  whenever i go to a place that doesn’t take app...  negative   \n",
            "4  apple pay is so convenient secure and easy to ...  positive   \n",
            "\n",
            "  predicted_sentiment  \n",
            "0            positive  \n",
            "1            positive  \n",
            "2            positive  \n",
            "3            positive  \n",
            "4            positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset with predictions to a new file (New Addition)\n",
        "df.to_csv(\"YoutubeCommentsDataSet_with_predictions.csv\", index=False)#print(\"Predictions saved in 'YoutubeCommentsDataSet_with_predictions.csv'\")"
      ],
      "metadata": {
        "id": "kp_Ckw0J2k7w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment of new comments using the saved model\n",
        "def predict_sentiment(comment):\n",
        "    model, vectorizer = load_model()\n",
        "    clean_comment = clean_text(comment)\n",
        "    vectorized_comment = vectorizer.transform([clean_comment])\n",
        "    return model.predict(vectorized_comment)[0]\n"
      ],
      "metadata": {
        "id": "CafcmnR-wtTA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Prediction\n",
        "example_comment = \"This video is amazing! I loved it.\"\n",
        "print(\"Predicted Sentiment:\", predict_sentiment(example_comment))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_rjOsDww4Bu",
        "outputId": "2c61383b-d33e-4c85-b60b-a496f7052eaf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: positive\n"
          ]
        }
      ]
    }
  ]
}